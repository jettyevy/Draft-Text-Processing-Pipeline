{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sieji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sieji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "######################################### IMPORTING PACAKGES #############################\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import sys  \n",
    "import os\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "# PDF text extraction\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.pdfinterp import PDFResourceManager\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer3.converter import PDFPageAggregator\n",
    "from pdfminer3.converter import TextConverter\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# Others\n",
    "import requests\n",
    "import string\n",
    "import re\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(stop_words)\n",
    "\n",
    "DATA_FOLDER = \"dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Process raw PDF text to structured and processed PDF text to be worked on in Python.\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : Relative Location of File\n",
    "    Return\n",
    "    ------\n",
    "    text : str\n",
    "        processed PDF text if no error is throw\n",
    "    \"\"\"   \n",
    "\n",
    "    try:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        codec = 'utf-8'\n",
    "        laparams = LAParams()\n",
    "\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, codec=codec, laparams=laparams)\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        password = \"\"\n",
    "        maxpages = 0\n",
    "        caching = True\n",
    "        pagenos = set()\n",
    "\n",
    "        content = []\n",
    "\n",
    "        with open(file_path, 'rb') as file:\n",
    "            for page in PDFPage.get_pages(file,\n",
    "                                        pagenos, \n",
    "                                        maxpages=maxpages,\n",
    "                                        password=password,\n",
    "                                        caching=True,\n",
    "                                        check_extractable=False):\n",
    "\n",
    "                page_interpreter.process_page(page)\n",
    "\n",
    "                content.append(fake_file_handle.getvalue())\n",
    "\n",
    "                fake_file_handle.truncate(0)\n",
    "                fake_file_handle.seek(0)        \n",
    "\n",
    "        text = '##PAGE_BREAK##'.join(content)\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_pdf('dataset/BMW Sustainability Report 2019.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_pdf('Test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp preprocessing\n",
    "def preprocess_lines(line_input):\n",
    "    \"\"\"\n",
    "    Helper Function to preprocess and clean sentences from raw PDF text \n",
    "    Parameters\n",
    "    ----------\n",
    "    line_input : str\n",
    "        String that contains a sentence to be cleaned\n",
    "    Return\n",
    "    ------\n",
    "    line : str\n",
    "        Cleaned sentence\n",
    "    ----------\n",
    "    Sub: Substitute regular expression\n",
    "    Split: Remove blank space from front and rear \n",
    "    \"\"\"  \n",
    "    # removing header number\n",
    "    line = re.sub(r'^\\s?\\d+(.*)$', r'\\1', line_input)\n",
    "    # removing trailing spaces\n",
    "    line = line.strip()\n",
    "    # words may be split between lines, ensure we link them back together\n",
    "    line = re.sub(r'\\s?-\\s?', '-', line)\n",
    "    # remove space prior to punctuation\n",
    "    line = re.sub(r'\\s?([,:;\\.])', r'\\1', line)\n",
    "    # ESG contains a lot of figures that are not relevant to grammatical structure\n",
    "    line = re.sub(r'\\d{5,}', r' ', line)\n",
    "    # remove emails\n",
    "    line = re.sub(r'\\S*@\\S*\\s?', '', line)\n",
    "    # remove mentions of URLs\n",
    "    line = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', r' ', line)\n",
    "    # remove multiple spaces\n",
    "    line = re.sub(r'\\s+', ' ', line)\n",
    "    # join next line with space\n",
    "    line = re.sub(r' \\n', ' ', line)\n",
    "    line = re.sub(r'.\\n', '. ', line)\n",
    "    line = re.sub(r'\\x0c', ' ', line)\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of url from text\n",
    "# To be write later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To achieve this, we set ourselves ten ambitious goals along the entire value chain. The BMW Group Sustainable Value Report (SVR) has been published to provide stakeholders with comprehensive information about the company’s sustainability strategy and the progress made in integrating sustainability into its corporate processes.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_lines(\"To \\nachieve this, we set ourselves ten ambitious goals along the \\nentire value chain.\\nThe BMW Group Sustainable Value Report (SVR) has been \\npublished to provide stakeholders with comprehensive \\ninformation about the company’s sustainability strategy \\nand the progress made in integrating sustainability into \\nits corporate processes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##PAGE_BREAK##14 Introduction 1 Fundamentals · 1.1 Strategy 1.2 Sustainability management 1.3 Stakeholder dialogue 1.4 Compliance and human rights 1.5 Product safety 2 Products and services 3 Production and value creation 4 Employees and society Appendix BMW Group’s position on the recommendations of the Task Force on Climate-related Financial Disclosures Climate change is one of the greatest social challenges of our time'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_lines(\"\\n\\n\\x0c##PAGE_BREAK##14\\n\\nIntroduction\\n\\n1\\n\\nFundamentals\\n\\n·\\n\\n1.1  Strategy\\n1.2  Sustainability management\\n1.3  Stakeholder dialogue\\n1.4  Compliance and human rights\\n1.5  Product safety\\n\\n2\\nProducts and services\\n\\n3\\nProduction and \\nvalue creation\\n\\n4\\nEmployees and society\\n\\nAppendix\\n\\nBMW Group’s position on the recommendations of the \\n Task Force on Climate-related Financial Disclosures\\nClimate change is one of the greatest social challenges of \\nour time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    \"\"\"\n",
    "    Helper Function to remove non ascii characters from text\n",
    "    Printable will \n",
    "    \"\"\"\n",
    "    printable = set(string.printable) #Convert iterable to set\n",
    "    return ''.join(filter(lambda x: x in printable, text))\n",
    "\n",
    "def not_header(line):\n",
    "    \"\"\"\n",
    "    Helper Function to remove headers\n",
    "    Check if all the characters are in upper case\n",
    "    \"\"\"\n",
    "    return not line.isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#@#$Jjsjnfsjnf#@$'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_non_ascii(\"#@#$Jjsjnfsjnf#@$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_header('HELLO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pages_sentences(nlp, text):    \n",
    "    \"\"\"\n",
    "    Extracting text from raw PDF text and store them by pages and senteces. Raw text is also cleand by removing junk, URLs, etc.\n",
    "    Consecutive lines are also grouped into paragraphs and spacy is used to parse sentences.\n",
    "    Parameters\n",
    "    ----------\n",
    "    nlp: spacy nlp model\n",
    "        NLP model to parse sentences\n",
    "    text : str\n",
    "        Raw PDF text\n",
    "    Return\n",
    "    ------\n",
    "    pages_content : list of str\n",
    "        A list containing text from each page of the PDF report. Page number is the index of list + 1\n",
    "    \n",
    "    pages_sentences : list of list\n",
    "        A list containing lists. Page number is the index of outer list + 1. Inner list contains sentences from each page\n",
    " \n",
    "    \"\"\"  \n",
    "    MIN_WORDS_PER_PAGE = 500\n",
    "    \n",
    "    pages = text.split('##PAGE_BREAK##')\n",
    "    #print('Number of Pages: {}'.format(len(pages)))\n",
    "\n",
    "    lines = []\n",
    "    for i in range(len(pages)):\n",
    "        page_number = i + 1\n",
    "        page = pages[i]\n",
    "        \n",
    "        # remove non ASCII characters\n",
    "        text = remove_non_ascii(page)\n",
    "        \n",
    "        # if len(text.split(' ')) < MIN_WORDS_PER_PAGE:\n",
    "        #     print(f'Skipped Page: {page_number}')\n",
    "        #     continue\n",
    "        \n",
    "        prev = \"\"\n",
    "        for line in text.split('\\n\\n'):\n",
    "            # aggregate consecutive lines where text may be broken down\n",
    "            # only if next line starts with a space or previous does not end with dot.\n",
    "            if(line.startswith(' ') or not prev.endswith('.')):\n",
    "                prev = prev + ' ' + line\n",
    "            else:\n",
    "                # new paragraph\n",
    "                lines.append(prev)\n",
    "                prev = line\n",
    "\n",
    "        # don't forget left-over paragraph\n",
    "        lines.append(prev)\n",
    "        lines.append('##SAME_PAGE##')\n",
    "        \n",
    "    lines = '  '.join(lines).split('##SAME_PAGE##')\n",
    "    \n",
    "    # clean paragraphs from extra space, unwanted characters, urls, etc.\n",
    "    # best effort clean up, consider a more versatile cleaner\n",
    "    \n",
    "    pages_content = []\n",
    "    pages_sentences = []\n",
    "\n",
    "    for line in lines[:-1]: # looping through each page\n",
    "        \n",
    "        line = preprocess_lines(line)       \n",
    "        pages_content.append(str(line).strip())\n",
    "\n",
    "        sentences = []\n",
    "        # split paragraphs into well defined sentences using spacy\n",
    "        for part in list(nlp(line).sents):\n",
    "            sentences.append(str(part).strip())\n",
    "\n",
    "        #sentences += nltk.sent_tokenize(line)\n",
    "            \n",
    "        # Only interested in full sentences and sentences with 10 to 100 words. --> filter out first page/content page\n",
    "        sentences = [s for s in sentences if re.match('^[A-Z][^?!.]*[?.!]$', s) is not None]\n",
    "        sentences = [s.replace('\\n', ' ') for s in sentences]\n",
    "        \n",
    "        pages_sentences.append(sentences)\n",
    "        \n",
    "    return pages_content, pages_sentences #list, list of list where page is index of outer list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_content, pages_sentences = extract_pages_sentences(spacy.load(\"en_core_web_sm\"),extract_pdf(\"Test.pdf\"))\n",
    "pages_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(report):\n",
    "    \"\"\"\n",
    "    Lemmatize,lowercase and remove stopwords for pages of a report\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    report: list of str\n",
    "        A list containing text from each page of the PDF report. Page number is the index of list + 1\n",
    "    Return\n",
    "    ------\n",
    "    report_pages : list of str\n",
    "        A list containing processed text from each page of the PDF report. Page number is the index of list + 1\n",
    "    \n",
    "    \"\"\"  \n",
    "    \n",
    "    report_pages = []\n",
    "\n",
    "    def para_to_sent(para):\n",
    "        \"\"\"\n",
    "        Helper function to split paragraphs into well defined sentences using spacy\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        for part in list(nlp(para).sents):\n",
    "            sentences.append(str(part).strip())\n",
    "        return sentences\n",
    "\n",
    "    def remove_stopwords(texts):\n",
    "        \"\"\"\n",
    "        Helper function to remove stopwords from sentence\n",
    "        \"\"\"\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        \"\"\"\n",
    "        Helper function to lemmatize text in sentence\n",
    "        \"\"\"\n",
    "        texts_out = []\n",
    "        doc = nlp(texts) \n",
    "        texts_out.append(\" \".join([token.lemma_ for token in doc]))\n",
    "        return texts_out\n",
    "\n",
    "    def stemming(texts):\n",
    "        stemmer = SnowballStemmer(language='english')\n",
    "        revisions = [stemmer.stem(text) for text in texts]\n",
    "        return revisions\n",
    "    \n",
    "    for page in report:\n",
    "\n",
    "        sentences = para_to_sent(page.lower())\n",
    "\n",
    "        # Do lemmatization keeping only noun, adj, vb, adv\n",
    "        page_data = []\n",
    "        for sentence in sentences : \n",
    "            data_lemmatized = lemmatization(sentence, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "            data_stemmed_lemmatized = stemming(sentences)\n",
    "            page_data.extend(data_stemmed_lemmatized)\n",
    "        page_para_stem_lemma = \"\".join(page_data)\n",
    "        \n",
    "        report_pages.append(page_para_stem_lemma)\n",
    "    \n",
    "    return report_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(pages_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text_list, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    # lemmatize text in sentence\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for texts in text_list:\n",
    "        texts = texts.lower()\n",
    "        texts_out.append(\" \".join([token.lemma_ for token in nlp(texts)]))\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_report_highLevel(report):\n",
    "    \"\"\"\n",
    "    Page filter to filter report for only relevant pages with decarbonisation related words.\n",
    "    Two types of word filters: direct and indirect. Direct contains words that are directly related to decarbonisation while indirect contains other relevant decarbonisation information.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    report: list of str\n",
    "        A list containing text from each page of the PDF report. Page number is the index of list + 1\n",
    "    Return\n",
    "    ------\n",
    "    filtered_report_direct : dict of {int : str}\n",
    "        A dictionary that contains relevant pages obtained using direct filter. The key is the page number and value is the text on the page. \n",
    "    \n",
    "    filtered_report_indirect : dict of {int : str}\n",
    "        A dictionary that contains relevant pages obtained using indirect filter. The key is the page number and value is the text on the page.     \n",
    "    \"\"\"  \n",
    "    \n",
    "    # list of words used to filter\n",
    "    relevant_terms_directFilter = set([\"carbon\",\"co2\",\"environment\",\"GHG emissions\",\"Greenhouse Gas\",\"carbon footprint\",\"carbon emissions\",\"Scope 1\",\"Scope 2\",\n",
    "                               \"Scope 3\", \"WACI\",\"Carbon Intensity\",\"carbon pricing\",\"net-zero\",\"metrics and targets\",\"TCFD\",\n",
    "                                \"sustainability goals\",\"decarbonisation\",\"climate\",'energy', 'emission', 'emissions', 'renewable', 'carbon', 'fuel', 'power', \n",
    "                               'green', 'gas', 'green energy', 'sustainable', 'climate', 'sustainability', 'environmental', 'environment', 'GHG', \n",
    "                               'decarbon', 'energy consumption', 'paper consumption','water consumption', 'carbon intensity', 'waste management', 'electricity consumption', \n",
    "                                'cdp', 'global warming', 'business travel','climate solutions', 'decarbonization', 'cvar', 'climate value-at-risk','waste output'])\n",
    "    relevant_terms_combinationA = [\"emissions\",\"exposure\",\"carbon related\",\"esg\",\"sustainable\",\"green\",\"climate sensitive\",\"impact investing\", \"investment framework\", 'msci', 'ftse', 'responsible investing', 'responsible investment','transition']\n",
    "    relevant_terms_combinationB = [\"portfolio\",\"assets\",\"AUM\",\"investment\",\"financing\",\"ratings\",\"revenue\",\"bond\",\"goal\",\"insurance\", \"equity\", \"swap\", \"option\", \"portfolio holdings\", \"risk management\",'financial products']\n",
    "    relevant_terms_combinationC = [\"net zero\",\"carbon footprint\",\"CO2\",\"carbon\",\"oil\",\"coal\", \"gas\", \"fossil fuel\",\"green\"]\n",
    "    relevant_terms_combination_directFilter_lem = lemmatization(relevant_terms_directFilter)\n",
    "    relevant_terms_combinationA_lem = lemmatization(relevant_terms_combinationA)\n",
    "    relevant_terms_combinationB_lem = lemmatization(relevant_terms_combinationB)\n",
    "    relevant_terms_combinationC_lem = lemmatization(relevant_terms_combinationC)\n",
    "    \n",
    "    \n",
    "    filtered_report_direct = {}\n",
    "    filtered_report_indirect = {}\n",
    "    for i in range(len(report)):\n",
    "        page = report[i]\n",
    "        page_number = i + 1\n",
    "        no_words = len(page.split(\" \"))\n",
    "        \n",
    "        # filter for pages that contain at least 3 words from the relevant_terms_combination_directFilter_lem list\n",
    "        if sum(map(page.__contains__, relevant_terms_combination_directFilter_lem)) > 2:\n",
    "            filtered_report_direct[page_number] = page\n",
    "        \n",
    "        # filter for pages that contain at least 1 word (relevant_terms_combinationC_lem AND relevant_terms_combinationA_lem) OR (relevant_terms_combinationC_lem AND  relevant_terms_combinationB_lem)\n",
    "        elif (any(map(page.__contains__, relevant_terms_combinationA_lem)) and any(map(page.__contains__, relevant_terms_combinationC_lem))) or (any(map(page.__contains__, relevant_terms_combinationB_lem)) and any(map(page.__contains__, relevant_terms_combinationC_lem))):\n",
    "            filtered_report_indirect[page_number] = page\n",
    "    \n",
    "    return filtered_report_direct,filtered_report_indirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_report_direct, filtered_report_indirect = filter_report_highLevel(pages_content)\n",
    "filtered_report_direct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lines_2(line_input):\n",
    "    # removing header number\n",
    "    line = re.sub(r'^\\s?\\d+(.*)$', r'\\1', line_input)\n",
    "    # removing trailing spaces\n",
    "    line = line.strip()\n",
    "    # words may be split between lines, ensure we link them back together\n",
    "    line = re.sub(r'\\s?-\\s?', '-', line)\n",
    "    # remove space prior to punctuation\n",
    "    line = re.sub(r'\\s?([,:;\\.])', r'\\1', line)\n",
    "    # ESG contains a lot of figures that are not relevant to grammatical structure\n",
    "    line = re.sub(r'\\d{5,}', r' ', line)\n",
    "    # remove emails\n",
    "    line = re.sub(r'\\S*@\\S*\\s?', '', line)\n",
    "    # remove mentions of URLs\n",
    "    line = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', r' ', line)\n",
    "    # remove multiple spaces\n",
    "    line = re.sub(r'\\s+', ' ', line)\n",
    "\n",
    "\n",
    "    # remove punctuation\n",
    "    line = re.sub(r'[,\\.!?]', '', line)\n",
    "    # convert to lowercase\n",
    "    line = line.lower()\n",
    "    \n",
    "    # join next line with space\n",
    "    line = re.sub(r' \\n', ' ', line)\n",
    "    line = re.sub(r'.\\n', '. ', line)\n",
    "    line = re.sub(r'\\x0c', ' ', line)\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the following ghg emissions are not included in the net carbon intensity (nci): ■ emissions from production processing use and end-of-life treatment of non-energy products such as chemicals and lubricants; ■ emissions from third-party processing of sold intermediate products such as the manufacture of plastics from feedstocks sold by shell; ■ emissions associated with the construction and decommissioning of production and manufacturing facilities; ■ emissions associated with the production of fuels purchased to generate energy on site at a shell facility; ■ other indirect emissions from waste generated in operations business travel employee commuting transmission and distribution losses associated with imported electricity franchises and investments; ■ emissions from capital goods defined by the ghg protocol as including fixed assets or property plant and equipment (pp&e) and other goods and services not related to purchased energy feedstocks sourced from third parties or energy products manufactured by third parties and sold by shell'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_lines_2(\"The following GHG emissions are not included in the net carbon \\nintensity\\xa0(NCI):\\n ■ emissions from production, processing, use and end-of-life treatment \\n\\nof\\xa0non-energy products, such as chemicals and lubricants;\\n\\n ■ emissions from third-party processing of sold intermediate products, \\n\\nsuch as the manufacture of plastics from feedstocks sold by Shell;\\n ■ emissions associated with the construction and decommissioning \\n\\nof\\xa0production and manufacturing facilities;\\n\\n ■ emissions associated with the production of fuels purchased \\n\\nto\\xa0generate energy on site at a Shell facility;\\n\\n ■ other indirect emissions from waste generated in operations, business \\n\\ntravel, employee commuting, transmission and distribution losses \\nassociated with imported electricity, franchises and investments;\\n\\n ■ emissions from capital goods, defined by the GHG Protocol as \\n\\nincluding fixed assets or property, plant and equipment (PP&E), and \\nother goods and services not related to purchased energy feedstocks \\nsourced from third parties or energy products manufactured by third \\nparties and sold by Shell.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9663fbe026b7cd2ea4f67c5081e3e9f4142d3139d83ebc8aef8d45115642e76f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
